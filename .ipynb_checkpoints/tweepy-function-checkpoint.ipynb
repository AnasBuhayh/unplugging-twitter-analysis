{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source \n",
    "# https://python.plainenglish.io/scraping-tweets-with-tweepy-python-59413046e788\n",
    "\n",
    "# Import the libraries\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "from datetime import datetime\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re #regular expression\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import preprocessor as p\n",
    "import os\n",
    "import time\n",
    "import keys\n",
    "\n",
    "# Authenticating Twitter API\n",
    "# Obtain your Twitter credentials from your twitter developer account\n",
    "\n",
    "consumer_key = keys.CONSUMER_KEY\n",
    "consumer_secret = keys.CONSUMER_SECRET\n",
    "access_key = keys.ACCESS_KEY\n",
    "access_secret = keys.ACCESS_SECRET\n",
    "\n",
    "# Pass your twitter credentials to tweepy via its OAuthHandler\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Automating Scraping\n",
    "# Calls API every 15 minutes to prevent overcalling\n",
    "\n",
    "# 1. define a for-loop\n",
    "# 2. define search parameter\n",
    "# 3. define date period\n",
    "# 4. define no. of tweets to pull\n",
    "\n",
    "def scraptweets(search_words, date_since, numTweets, numRuns, f_name, language, time_sleep):\n",
    "\n",
    "    ## Arguments:\n",
    "    # search_words -> define a string of keywords for this function to extract\n",
    "    # date_since -> define a date from which to start extracting the tweets \n",
    "    # numTweets -> number of tweets to extract per run\n",
    "    # numRun -> number of runs to perform in this program - API calls are limited to once every 15 mins, so each run will be 15 mins apart.\n",
    "    ##\n",
    "    \n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['tweet_id', 'username', 'acctdesc', 'location', 'following',\n",
    "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "                                        'retweetcount', 'text', 'hashtags']\n",
    "                                )\n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    for i in range(0, numRuns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        # since=date_since, \n",
    "        tweets = tweepy.Cursor(api.search, q=search_words, lang=language, tweet_mode='extended').items(numTweets)\n",
    "\n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "\n",
    "        # Obtain the following info (methods to call them out):\n",
    "            # user.screen_name - twitter handle\n",
    "            # user.description - description of account\n",
    "            # user.location - where is he tweeting from\n",
    "            # user.friends_count - no. of other users that user is following (following)\n",
    "            # user.followers_count - no. of other users who are following this user (followers)\n",
    "            # user.statuses_count - total tweets by user\n",
    "            # user.created_at - when the user account was created\n",
    "            # created_at - when the tweet was created\n",
    "            # retweet_count - no. of retweets\n",
    "            # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
    "            # retweeted_status.full_text - full text of the tweet\n",
    "            # tweet.entities['hashtags'] - hashtags in the tweet\n",
    "\n",
    "        # Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "\n",
    "        for tweet in tweet_list:\n",
    "\n",
    "            # Pull the values\n",
    "            tweet_id = tweet.id\n",
    "            username = tweet.user.screen_name\n",
    "            acctdesc = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            usercreatedts = tweet.user.created_at\n",
    "            tweetcreatedts = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "\n",
    "            try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "            except AttributeError:  # Not a Retweet\n",
    "                text = tweet.full_text\n",
    "\n",
    "            # Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [tweet_id, username, acctdesc, location, following, followers, totaltweets,\n",
    "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
    "\n",
    "            # Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "\n",
    "            # increase counter - noTweets  \n",
    "            noTweets += 1\n",
    "        \n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        duration_run = round(end_run-start_run, 2)\n",
    "        \n",
    "        #timestamp\n",
    "        run_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        print('run {} start time: {}'.format(i, run_timestamp))\n",
    "        print('no. of tweets scraped for run {} is {}'.format(i, noTweets))\n",
    "        print('time take for {} run to complete is {}'.format(i, duration_run))\n",
    "        \n",
    "        if not (i == numRuns-1):\n",
    "            time.sleep(time_sleep) #15 minute sleep time (default: 900)\n",
    "\n",
    "              \n",
    "    # Once all runs have completed, save them to a single csv file:    \n",
    "    # Obtain timestamp in a readable format:\n",
    "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    # Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/data/' + to_csv_timestamp + f_name + \"_noTweets=\" + str(noTweets) + '.csv'\n",
    "\n",
    "    # Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = \n",
    "                     False, encoding='utf-8-sig')\n",
    "\n",
    "    print('Scraping completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 start time: 20210811_160306\n",
      "no. of tweets scraped for run 0 is 399\n",
      "time take for 0 run to complete is 26.21\n",
      "run 1 start time: 20210811_161925\n",
      "no. of tweets scraped for run 1 is 399\n",
      "time take for 1 run to complete is 79.39\n",
      "run 2 start time: 20210811_163452\n",
      "no. of tweets scraped for run 2 is 399\n",
      "time take for 2 run to complete is 27.03\n",
      "run 3 start time: 20210811_165017\n",
      "no. of tweets scraped for run 3 is 399\n",
      "time take for 3 run to complete is 24.86\n",
      "run 4 start time: 20210811_170605\n",
      "no. of tweets scraped for run 4 is 399\n",
      "time take for 4 run to complete is 47.94\n",
      "Scraping completed!\n"
     ]
    }
   ],
   "source": [
    "# Initialise these variables:\n",
    "\n",
    "# To search by country\n",
    "# place_id = api.geo_search(query=\"libya\", granularity=\"country\")[0].id\n",
    "# search_words = \"place:%s\" % place_id\n",
    "\n",
    "search_words = \"social media break\" \n",
    "date_since = \"2021-08-03\"\n",
    "numTweets = 399\n",
    "numRuns = 5\n",
    "f_name = \"_break_collection\"\n",
    "language = \"en\"\n",
    "time_sleep = 900\n",
    "# Call the function scraptweets\n",
    "scraptweets(search_words, date_since, numTweets, numRuns, f_name, language, time_sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"social media break OR break from social media OR disconnect to reconnect OR Digital detox  OR social media fast OR unplugging social media OR social media addiction OR time off social media OR social media detox\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
